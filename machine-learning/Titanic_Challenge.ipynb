{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqJSQzmVDzrx"
   },
   "source": [
    "# The Big Titanic Challenge - Who survived, who died?\n",
    "\n",
    "It's time to get our hands dirty and do some Machine Learning!\n",
    "Who will find the best model to predict survival and death on the Titanic? Let's find out...\n",
    "\n",
    "## Shoutouts:\n",
    "\n",
    "This homework is based on the Titanic challenge on Kaggle: https://www.kaggle.com/c/titanic \n",
    "\n",
    "## General Description (taken from Kaggle)\n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc).\n",
    "\n",
    "## The Task\n",
    "\n",
    "This notebook has several parts that correspond to important steps of the Machine Learning pipeline: Selecting a good model, eliminating useless features, and tuning hyperparameters. There's a lot of assistance and much of the work is already done for you. Still, you will have a lot of opportunities to try out things and improve your models.\n",
    "\n",
    "The overall goal is to find the best possible model to predict death and survival for a **test set of unseen data**. We will use accuracy (i.e., the percentage of our predictions that were correct) as our perfomance measure.\n",
    "\n",
    "**Once you're done, post your high score on the [leader board on Miro](https://miro.com/app/board/o9J_kplKUVg=/)!**\n",
    "\n",
    "Have fun, play around, and squeeze those last percentage points ;-)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To run this notebook you should:\n",
    "- run on Python 3.6\n",
    "- have numpy, pandas and sklearn installed (e.g., by using pip or anaconda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1I2bgzDo_D9"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1601899681179,
     "user": {
      "displayName": "Jakob Schmitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiGVbmP93XWblyZ6RLS0w-gzG3RI8X4zVw_WMCT=s64",
      "userId": "17501418531882160141"
     },
     "user_tz": -120
    },
    "id": "V_JHLCwSDzr1"
   },
   "outputs": [],
   "source": [
    "# Data Processing \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# ML Algorithms\n",
    "import sklearn \n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45krPG9rDzr7"
   },
   "source": [
    "## The dataset\n",
    "\n",
    "For this exercise we will load data that has been preprocessed already. It is based on the data from the Kaggle challenge, but has been processed to make it easier to work with.\n",
    "\n",
    "Here are the features of this dataset:\n",
    "* Survived: 0=died, 1=survived\n",
    "* Sex: 0=male, 1=female\n",
    "* Embarked: Port of embarkment, 0=Southhampton, 1=Cherbourg, 2=Queenstown\n",
    "* Title: 0=Mr, 1=Mrs, 2=Miss, 3=Master, 4=Royalty, 5=Officer\n",
    "* AgeBin: 0=Baby, 1=Child, 2=Teenager, 3=Young Adult, 4=Adult, 5=Senior\n",
    "* Family_Alone {Family_Small} [Family_Large]: 1=Passenger travelled alone {with 1 to 3 family members aboard} [with more than 4 family members aboard], 0=otherwise\n",
    "* Pclass_ordinal: Ticket class, 1=1st class, 2=2nd class, 3=3rd class\n",
    "* The rest is [dummy codings](https://en.wikiversity.org/wiki/Dummy_variable_(statistics)) of Pclass_ordinal, AgeBin, and Title. Wonder why one dummy is always missing? Read [here](https://www.algosome.com/articles/dummy-variable-trap-regression.html) about the dummy variable trap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 1455,
     "status": "ok",
     "timestamp": 1601899916954,
     "user": {
      "displayName": "Jakob Schmitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiGVbmP93XWblyZ6RLS0w-gzG3RI8X4zVw_WMCT=s64",
      "userId": "17501418531882160141"
     },
     "user_tz": -120
    },
    "id": "iHQ0WyyiDzr9",
    "outputId": "7508601b-7f83-457b-8db6-b8ff0e849f7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>AgeBin</th>\n",
       "      <th>Family_Alone</th>\n",
       "      <th>Family_Small</th>\n",
       "      <th>Family_Large</th>\n",
       "      <th>Pclass_ordinal</th>\n",
       "      <th>...</th>\n",
       "      <th>Title_1</th>\n",
       "      <th>Title_2</th>\n",
       "      <th>Title_3</th>\n",
       "      <th>Title_4</th>\n",
       "      <th>Title_5</th>\n",
       "      <th>Age_1</th>\n",
       "      <th>Age_2</th>\n",
       "      <th>Age_3</th>\n",
       "      <th>Age_4</th>\n",
       "      <th>Age_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>584</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>695</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Survived  Sex  Embarked  Title  AgeBin  Family_Alone  \\\n",
       "584         584         0    0         1      0       3             1   \n",
       "246         246         0    1         0      2       3             1   \n",
       "695         695         0    0         0      0       4             1   \n",
       "582         582         0    0         0      0       4             1   \n",
       "270         270         0    0         0      0       4             1   \n",
       "\n",
       "     Family_Small  Family_Large  Pclass_ordinal  ...  Title_1  Title_2  \\\n",
       "584             0             0               3  ...        0        0   \n",
       "246             0             0               3  ...        0        1   \n",
       "695             0             0               2  ...        0        0   \n",
       "582             0             0               2  ...        0        0   \n",
       "270             0             0               1  ...        0        0   \n",
       "\n",
       "     Title_3  Title_4  Title_5  Age_1  Age_2  Age_3  Age_4  Age_5  \n",
       "584        0        0        0      0      0      1      0      0  \n",
       "246        0        0        0      0      0      1      0      0  \n",
       "695        0        0        0      0      0      0      1      0  \n",
       "582        0        0        0      0      0      0      1      0  \n",
       "270        0        0        0      0      0      0      1      0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read train and test data into Pandas DataFrames\n",
    "data = pd.read_csv(\"/Users/suadkamardeen/Documents/CODE/SE/SE14 - AI Basics/machine-learning/AI_Guild_Titanic_Data.csv\")\n",
    "data.sample(5)\n",
    "\n",
    "data.drop(['FareClass'], axis=1, inplace=True)\n",
    "\n",
    "# print five random data points as example\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bm7tx0gDzsF"
   },
   "source": [
    "The loaded data is part of the training data from the Kaggle challenge. \n",
    "\n",
    "We'll do some more preprocessing steps here:\n",
    "- Reduce the dimensionality of the data (pick specific features only)\n",
    "- Split into input (X) and output(y) data.\n",
    "- Split into training data (X_train, y_train) and test data (X_test, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 649,
     "status": "ok",
     "timestamp": 1601899941269,
     "user": {
      "displayName": "Jakob Schmitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiGVbmP93XWblyZ6RLS0w-gzG3RI8X4zVw_WMCT=s64",
      "userId": "17501418531882160141"
     },
     "user_tz": -120
    },
    "id": "xSQcnWyADzsG"
   },
   "outputs": [],
   "source": [
    "# Pick the features we would like to use for our model\n",
    "feature_cols = [\"Sex\", \"Embarked\", \"Title\", \"AgeBin\", \"Family_Alone\", \"Family_Small\", \"Family_Large\", \"Pclass_ordinal\"]\n",
    "\n",
    "# split into input (X) and output (y)\n",
    "X, y = data[feature_cols], data[\"Survived\"]\n",
    "\n",
    "# Automatically split the data into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NGUsJqhDzsL"
   },
   "source": [
    "## A simple model: Logistic Regression\n",
    "\n",
    "Now let's train a simple model: Logistic Regression.\n",
    "Since we are using sklearn, this is quite easy, as the most common machine learning models have already been implemented. So all we have to do is:\n",
    "- Instantiate the model and configure it with [hyper-parameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning))\n",
    "- train the model\n",
    "- print out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1601899943937,
     "user": {
      "displayName": "Jakob Schmitt",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiGVbmP93XWblyZ6RLS0w-gzG3RI8X4zVw_WMCT=s64",
      "userId": "17501418531882160141"
     },
     "user_tz": -120
    },
    "id": "HqWNVVknDzsO",
    "outputId": "313fc6ef-435a-4018-c9bc-f93aa5942383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (Training Set): 83.29 %\n",
      "\n",
      "Prediction accuracy (Test Set): 79.89 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model and its hyper parameters\n",
    "model = LogisticRegression(C=1.0, max_iter=1000)\n",
    "\n",
    "# train the model \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# print trainin set accuracy\n",
    "train_acc_log = round(model.score(X_train, y_train) * 100, 2)\n",
    "print(\"Prediction accuracy (Training Set):\", train_acc_log, \"%\\n\")\n",
    "\n",
    "# Check and print prediction accuracy and model parameters\n",
    "test_acc_log = round(model.score(X_test, y_test) * 100, 2)\n",
    "print(\"Prediction accuracy (Test Set):\", test_acc_log, \"%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ08GfNnDzsX"
   },
   "source": [
    "## Your Playground\n",
    "\n",
    "### EASY: Play around with different models\n",
    "\n",
    "Here is a place for you to play around and find a machine learning model that performs well. Funnily enough, as long as you stick with sklearn this mostly means altering one specific line from the logistic regression to specify a different combination of model and hyper parameters.\n",
    "\n",
    "Here is a link of all supervised machine learning models sklearn has to offer: https://scikit-learn.org/stable/supervised_learning.html\n",
    "\n",
    "Keep in mind that this is a classification problem. Regression problems will need a bit of extra work to apply.\n",
    "\n",
    "Also, remember that you have to import the models you want to use first.\n",
    "\n",
    "Have fun :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Hl1JMOTyDzsZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (Training Set): 83.99 %\n",
      "\n",
      "Prediction accuracy (Test Set): 80.45 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model and its hyper parameters\n",
    "model = svm.SVC()\n",
    "\n",
    "# train the model \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# print trainin set accuracy\n",
    "train_acc_log = round(model.score(X_train, y_train) * 100, 2)\n",
    "print(\"Prediction accuracy (Training Set):\", train_acc_log, \"%\\n\")\n",
    "\n",
    "# Check and print prediction accuracy \n",
    "test_acc_log = round(model.score(X_test, y_test) * 100, 2)\n",
    "print(\"Prediction accuracy (Test Set):\", test_acc_log, \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR_UR9C5o_EX"
   },
   "source": [
    "### MEDIUM: Automatic Feature Selection\n",
    "\n",
    "To prevent overfitting, only the most important features should be used within a model. When selecting an optimal set of features, we seek to find the sweet spot between high variance and high bias. In other words, we don't want to throw away relevant information, but neither do we want to include spurious relations.\n",
    "\n",
    "In Recursive Feature Elimination with Cross Validation (RFECV), features are ranked according to their importance for making predictions. Then, two steps are repeated various times: 1. Models are trained and cross-validated (e.g., using accuracy scores as performance metric), 2. The least important feature is eliminated. In the end, the feature set leading to the best average out-of-sample prediction performance is automatically selected.\n",
    "\n",
    "If you're up for a little challenge, look at the documentation for [Scikit's own RFECV module](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html) and try to implement it for one of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kasOBXiyo_EY"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "model =  # TODO: Add your model here\n",
    "\n",
    "# Create RFECV instance and let it do the work (TODO: fill out the gaps accordingly)\n",
    "selector = RFECV(estimator=..., scoring=\"accuracy\")  \n",
    "selector.fit(...)  \n",
    "\n",
    "# The rest of this cell was written for your convenience\n",
    "# No need to modify it\n",
    "\n",
    "# Print the selected features\n",
    "support = selector.support_ # List of Booleans: Is feature selected?\n",
    "optimal_features = np.array(feature_cols)[support] # Unordered list of selected features\n",
    "print(\"Selected features:\", optimal_features)\n",
    "\n",
    "# Check and print prediction accuracy after automatic feature selection\n",
    "test_acc = model.fit(X_train[optimal_features], y_train) \\\n",
    "                .score(X_test[optimal_features], y_test)\n",
    "print(\"Out-of-sample prediction accuracy:\", round(test_acc * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYEqR1m6o_Ed"
   },
   "source": [
    "### HARD: Hyperparameter Tuning with Grid Search\n",
    "\n",
    "Hyperparameters (HPs) are meta settings of an ML algorithm that are not learned, but need to be specified before the learning process even starts. Examples of HPs for the simple logistic regression model that you dealt with above are the regularization parameter C and the maximum numbers of iterations during the optimization process, max_iter. You can find many more HPs [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "Most algorithms offer a myriad of HPs, and for optimal model performance, we are interested in finding the best combination of HPs. The most popular methods for automatic HP optimization (or 'tuning') are exhaustive grid search and random search. Those are very straight-forward and model-free methods. \n",
    "\n",
    "In [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), we just define for each HP which values we're interested in and then check all the possible combinations of HP values! This can take some time, but we'll be sure to find the best solution out of those tested. \n",
    "\n",
    "Now it's your turn again: Take a simple model (like logistic regression), pick at least two hyperparameters (like C and max_iter), define the HP search spaces, and implement grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLhALGXgo_Ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# TODO: Define search space for at least two hyperparameters\n",
    "search_space = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    ...\n",
    "}\n",
    "\n",
    "# Run automatic HP tuning (TODO: Fill the gaps accordingly)\n",
    "classifier = GridSearchCV(estimator=..., param_grid=...)\n",
    "classifier.fit(...)\n",
    "\n",
    "# The rest of this cell was written for your convenience\n",
    "# No need to modify it\n",
    "\n",
    "# Print the selected HP values\n",
    "print(classifier.best_params_)\n",
    "\n",
    "# Check and print prediction accuracy after automatic feature selection\n",
    "test_acc = classifier.best_estimator_.score(X_test, y_test)\n",
    "print(\"Out-of-sample prediction accuracy:\", round(test_acc * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u70AdRKo_Eh"
   },
   "source": [
    "### VERY HARD: Hyperparameter Tuning with Random Search\n",
    "\n",
    "Apart from Grid Search, another algorithm to perform automatic hyperparameter tuning is called Random Search.\n",
    "\n",
    "In random search, we repeatedly pick a random value for each HP and then test the resulting combination. We do this n times and go with the best performer. \n",
    "\n",
    "For this challenge, implement [Random Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) without further assistance! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPnOALyco_Ei"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# TODO: Implement Random Search for a model of your choice.\n",
    "# Tune at least three hyperparameters and determine the out-of-sample prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAGTcopNo_Em"
   },
   "source": [
    "## How did you do??\n",
    "\n",
    "What was the best model you found? Post your high-score in the leader board on Miro and be prepared to explain briefly how you arrived there :-) Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMUfIy4-o_Em"
   },
   "source": [
    "### BONUS: Evaluation Metrics\n",
    "\n",
    "How well are our models doing? That's a super important question which is not always easy to answer! Throughout this notebook, we have (somewhat arbitrarily) picked a very simple evaluation metric, the accuracy (i.e., what percentage of our predictions were correct?).\n",
    "\n",
    "If you're curious and have some more time, read [this article](https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226) and find out about alternatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rR-kb88-o_En"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Titanic_Challenge.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
